[
  {
    "miniProjectId": "MP1",
    "name": "JS Event Loop Visualizer",
    "type": "lab",
    "purpose": "Build a concrete mental model of the JavaScript event loop by simulating it step by step instead of memorizing diagrams.",
    "details": "A minimal, browser-based visual simulator that models the JavaScript runtime queues: Call Stack, Microtask Queue, Macrotask Queue, and Web APIs. The lab executes scripted scenarios (setTimeout, Promise.then, async/await) and visualizes exactly when and why tasks move between queues.",
    "outcome": "Ability to predict execution order of async JavaScript confidently and explain why code behaves the way it does under the event loop.",
    "topicsCovered": ["T02"],
    "techStack": {
      "runtime": ["Browser JavaScript"],
      "tools": ["Chrome DevTools"],
      "libraries": [],
      "optional": []
    },
    "features": [
      {
        "featureId": "MP1-F1",
        "name": "Event loop state model",
        "description": "A simplified in-memory model representing Call Stack, Microtask Queue, Macrotask Queue, and Web APIs.",
        "topicsCovered": ["T02"],
        "implementationNotes": [
          "Represent each queue explicitly as a data structure",
          "Advance execution in discrete ticks (not real time)",
          "Never rely on actual JS scheduling for correctness"
        ],
        "pitfallsToAvoid": [
          "Letting real JS async behavior drive the simulation",
          "Merging microtasks and macrotasks conceptually"
        ]
      },
      {
        "featureId": "MP1-F2",
        "name": "Scripted execution scenarios",
        "description": "Predefined scripts demonstrating common async patterns (Promise.then, setTimeout, async/await).",
        "topicsCovered": ["T02"],
        "implementationNotes": [
          "Hardcode scenarios with expected execution order",
          "Advance one operation at a time",
          "Display current step and queue transitions"
        ],
        "pitfallsToAvoid": [
          "Random or user-generated scripts early",
          "Skipping explanation of why transitions happen"
        ]
      },
      {
        "featureId": "MP1-F3",
        "name": "Step-by-step execution controls",
        "description": "Controls to move execution forward one tick at a time.",
        "topicsCovered": ["T02"],
        "implementationNotes": [
          "Single-step execution button",
          "Highlight which queue is being processed",
          "Pause automatically between steps"
        ],
        "pitfallsToAvoid": [
          "Auto-running without explanation",
          "Real-time animation hiding causality"
        ]
      },
      {
        "featureId": "MP1-F4",
        "name": "Execution timeline output",
        "description": "A textual log showing execution order with timestamps or step numbers.",
        "topicsCovered": ["T02"],
        "implementationNotes": [
          "Log every dequeue and execution",
          "Clearly label microtask vs macrotask execution"
        ],
        "pitfallsToAvoid": [
          "Unlabeled logs",
          "Mixing conceptual and real timestamps"
        ]
      }
    ],
    "blog": {
      "title": "Event loop explained by building one",
      "gist": "Explain how the event loop actually works by walking through the simulator: why microtasks drain first, why setTimeout runs later than Promise.then, and why async/await is syntactic sugar over microtasks."
    },
    "executionGuidelines": [
      "Do not depend on real async APIs for scheduling correctness",
      "Model queues explicitly and deterministically",
      "Advance execution only via explicit steps",
      "Explain every transition in human-readable terms"
    ],
    "successCriteria": [
      "Can predict execution order of mixed Promise and setTimeout code",
      "Can explain why microtasks run before macrotasks",
      "Can debug async behavior without trial-and-error",
      "Can teach the event loop using the simulator alone"
    ],
    "status": {
      "started": false,
      "completed": false,
      "lastUpdated": null
    }
  },
  {
    "miniProjectId": "MP2",
    "name": "Memory Leak Finder",
    "type": "lab",
    "purpose": "Develop the skill to detect, reproduce, and explain JavaScript memory leaks using real tooling instead of guesswork.",
    "details": "A controlled lab that intentionally introduces multiple types of JavaScript memory leaks (closures, event listeners, detached DOM nodes, long-lived references). The lab focuses on capturing heap snapshots, analyzing retained object trees, and proving the root cause of leaks with evidence.",
    "outcome": "Ability to reliably identify memory leaks, explain why memory is retained, and verify fixes using heap snapshots and profiling tools.",
    "topicsCovered": ["T27"],
    "techStack": {
      "runtime": ["Browser JavaScript"],
      "tools": ["Chrome DevTools (Memory tab)"],
      "libraries": [],
      "optional": []
    },
    "features": [
      {
        "featureId": "MP2-F1",
        "name": "Intentional memory leak scenarios",
        "description": "A set of small, isolated scenarios that deliberately leak memory.",
        "topicsCovered": ["T27"],
        "implementationNotes": [
          "Create one leak per scenario (closure, event listener, DOM)",
          "Ensure leaks are reproducible and deterministic",
          "Allow triggering leaks multiple times"
        ],
        "pitfallsToAvoid": [
          "Multiple leaks in the same scenario",
          "Leaks that depend on timing or randomness"
        ]
      },
      {
        "featureId": "MP2-F2",
        "name": "Heap snapshot capture",
        "description": "Capture heap snapshots before and after triggering leaks to observe retained objects.",
        "topicsCovered": ["T27"],
        "implementationNotes": [
          "Take baseline snapshot",
          "Trigger leak repeatedly",
          "Take comparison snapshot"
        ],
        "pitfallsToAvoid": [
          "Comparing unrelated snapshots",
          "Skipping baseline measurements"
        ]
      },
      {
        "featureId": "MP2-F3",
        "name": "Retained object tree analysis",
        "description": "Analyze why objects are retained by inspecting retaining paths in DevTools.",
        "topicsCovered": ["T27"],
        "implementationNotes": [
          "Trace retainers back to root",
          "Identify which reference prevents GC",
          "Label the retention chain clearly"
        ],
        "pitfallsToAvoid": [
          "Assuming leaks without proving retention",
          "Stopping analysis at shallow references"
        ]
      },
      {
        "featureId": "MP2-F4",
        "name": "Leak fix and verification",
        "description": "Apply fixes and verify that memory is released correctly.",
        "topicsCovered": ["T27"],
        "implementationNotes": [
          "Remove references explicitly",
          "Unregister event listeners",
          "Re-run heap snapshot comparisons"
        ],
        "pitfallsToAvoid": [
          "Assuming a fix without verification",
          "Relying on page reloads to clear memory"
        ]
      }
    ],
    "blog": {
      "title": "How I hunted a JS memory leak",
      "gist": "Walk through a real memory leak investigation using heap snapshots: establishing a baseline, triggering leaks, analyzing retained object trees, and proving the fix with evidence instead of intuition."
    },
    "executionGuidelines": [
      "Never claim a memory leak without heap evidence",
      "Always compare snapshots taken under identical conditions",
      "Follow retention paths all the way to GC roots",
      "Verify fixes by demonstrating released memory"
    ],
    "successCriteria": [
      "Can identify different categories of memory leaks",
      "Can explain why specific objects are retained",
      "Can use heap snapshots and retained paths confidently",
      "Can prove that a fix actually releases memory"
    ],
    "status": {
      "started": false,
      "completed": false,
      "lastUpdated": null
    }
  },
  {
    "miniProjectId": "MP3",
    "name": "Cache Strategy Simulator",
    "type": "simulator",
    "purpose": "Build an intuition for how different caching strategies behave under real traffic and why cache invalidation is one of the hardest problems in systems.",
    "details": "A deterministic simulator that models clients, an application layer, a cache layer, and a backing store. The lab allows toggling cache strategies (cache-aside, write-through), TTL policies, stale-while-revalidate, and invalidation approaches, then visualizes hit rates, staleness, and load amplification.",
    "outcome": "Ability to choose appropriate caching strategies, explain freshness vs consistency tradeoffs, and predict failure modes like cache stampedes and stale data.",
    "topicsCovered": ["T23"],
    "techStack": {
      "runtime": ["Node.js"],
      "tools": ["Console-based metrics output"],
      "libraries": [],
      "optional": []
    },
    "features": [
      {
        "featureId": "MP3-F1",
        "name": "Cache strategy modes",
        "description": "Simulate cache-aside and write-through strategies against a backing store.",
        "topicsCovered": ["T23"],
        "implementationNotes": [
          "Explicitly model read/write paths for each strategy",
          "Track cache hits, misses, and write amplification"
        ],
        "pitfallsToAvoid": [
          "Implicit or mixed strategies",
          "Hiding cache behavior behind abstractions"
        ]
      },
      {
        "featureId": "MP3-F2",
        "name": "TTL and expiration policies",
        "description": "Apply different TTL values and observe staleness and load patterns.",
        "topicsCovered": ["T23"],
        "implementationNotes": [
          "Support short, long, and infinite TTLs",
          "Advance time deterministically in steps"
        ],
        "pitfallsToAvoid": [
          "Using real-time clocks",
          "Ignoring expiration edge cases"
        ]
      },
      {
        "featureId": "MP3-F3",
        "name": "Stale-while-revalidate",
        "description": "Serve stale data while asynchronously refreshing the cache.",
        "topicsCovered": ["T23"],
        "implementationNotes": [
          "Mark entries as stale vs expired",
          "Trigger background refresh without blocking reads"
        ],
        "pitfallsToAvoid": [
          "Blocking reads during refresh",
          "Unbounded background refreshes"
        ]
      },
      {
        "featureId": "MP3-F4",
        "name": "Invalidation strategies",
        "description": "Compare explicit invalidation, TTL-only, and write-based invalidation approaches.",
        "topicsCovered": ["T23"],
        "implementationNotes": [
          "Simulate write-triggered invalidation",
          "Track staleness window after writes"
        ],
        "pitfallsToAvoid": [
          "Assuming invalidation is instant",
          "Ignoring race conditions"
        ]
      },
      {
        "featureId": "MP3-F5",
        "name": "Cache stampede simulation",
        "description": "Demonstrate stampede scenarios when popular keys expire simultaneously.",
        "topicsCovered": ["T23"],
        "implementationNotes": [
          "Simulate concurrent reads after expiration",
          "Show load spike on backing store"
        ],
        "pitfallsToAvoid": [
          "Serializing reads unrealistically",
          "Ignoring jitter or request coalescing options"
        ]
      }
    ],
    "blog": {
      "title": "Cache invalidation is hard — here’s why",
      "gist": "Use the simulator to show how different cache strategies fail under writes, TTL expiry, and concurrency, and explain the unavoidable tradeoff between freshness, consistency, and performance."
    },
    "executionGuidelines": [
      "Model time deterministically; never rely on real timers",
      "Track metrics for cache hit rate, staleness, and backend load",
      "Simulate concurrency explicitly instead of assuming sequential access",
      "Keep the simulator minimal and focused on behavior, not UI"
    ],
    "successCriteria": [
      "Can explain cache-aside vs write-through tradeoffs",
      "Can demonstrate stale-while-revalidate behavior",
      "Can reproduce a cache stampede reliably",
      "Can justify TTL and invalidation choices with evidence"
    ],
    "status": {
      "started": false,
      "completed": false,
      "lastUpdated": null
    }
  },
  {
    "miniProjectId": "MP4",
    "name": "Retry Storm Simulator",
    "type": "simulator",
    "purpose": "Understand how retries meant to increase reliability can instead amplify failures and bring systems down.",
    "details": "A deterministic simulator that models clients, a service, and downstream dependencies under failure. The lab demonstrates how naive retry strategies (immediate retries, no jitter, no caps) can cause retry storms, overload services, and worsen outages. It also allows comparison with safer retry strategies.",
    "outcome": "Ability to design retry strategies that improve resilience without causing cascading failures or self-inflicted outages.",
    "topicsCovered": ["T24"],
    "techStack": {
      "runtime": ["Node.js"],
      "tools": ["Console-based metrics output"],
      "libraries": [],
      "optional": []
    },
    "features": [
      {
        "featureId": "MP4-F1",
        "name": "Naive retry strategy",
        "description": "Simulate immediate, unlimited retries on failure.",
        "topicsCovered": ["T24"],
        "implementationNotes": [
          "Retry immediately on failure",
          "No backoff, no jitter, no retry cap",
          "All clients retry simultaneously"
        ],
        "pitfallsToAvoid": [
          "Softening the retry behavior",
          "Accidentally adding delays or randomness"
        ]
      },
      {
        "featureId": "MP4-F2",
        "name": "Exponential backoff with jitter",
        "description": "Compare naive retries with exponential backoff and jitter.",
        "topicsCovered": ["T24"],
        "implementationNotes": [
          "Apply exponential delay between retries",
          "Add random jitter to spread retry load",
          "Set a maximum retry count"
        ],
        "pitfallsToAvoid": ["Using fixed delays only", "Unbounded retry loops"]
      },
      {
        "featureId": "MP4-F3",
        "name": "Retry amplification metrics",
        "description": "Measure how retries multiply traffic during failures.",
        "topicsCovered": ["T24"],
        "implementationNotes": [
          "Track original requests vs retry attempts",
          "Calculate amplification factor",
          "Visualize downstream request volume"
        ],
        "pitfallsToAvoid": [
          "Ignoring downstream saturation",
          "Only measuring client-side retries"
        ]
      },
      {
        "featureId": "MP4-F4",
        "name": "Idempotency impact",
        "description": "Demonstrate how retries interact with non-idempotent operations.",
        "topicsCovered": ["T24"],
        "implementationNotes": [
          "Simulate duplicate writes",
          "Track inconsistent state caused by retries",
          "Compare idempotent vs non-idempotent flows"
        ],
        "pitfallsToAvoid": [
          "Assuming retries are always safe",
          "Ignoring write-side effects"
        ]
      },
      {
        "featureId": "MP4-F5",
        "name": "Failure recovery window",
        "description": "Observe how systems recover (or fail to) after downstream service restoration.",
        "topicsCovered": ["T24"],
        "implementationNotes": [
          "Simulate recovery timing",
          "Measure time-to-stability after failure",
          "Compare retry strategies"
        ],
        "pitfallsToAvoid": [
          "Stopping simulation too early",
          "Ignoring delayed recovery effects"
        ]
      }
    ],
    "blog": {
      "title": "Retries can kill systems",
      "gist": "Use the simulator to show how naive retry logic causes retry storms, amplifies load, creates inconsistent state, and delays recovery — and how backoff, jitter, caps, and idempotency mitigate the damage."
    },
    "executionGuidelines": [
      "Model failures explicitly and deterministically",
      "Measure amplification, not just success rate",
      "Always compare retry strategies side by side",
      "Treat retries as part of system design, not client convenience"
    ],
    "successCriteria": [
      "Can explain why naive retries worsen outages",
      "Can demonstrate retry amplification numerically",
      "Can design safe retry policies with backoff and jitter",
      "Can explain retry + idempotency interactions clearly"
    ],
    "status": {
      "started": false,
      "completed": false,
      "lastUpdated": null
    }
  },
  {
    "miniProjectId": "MP5",
    "name": "Load Test Harness",
    "type": "lab",
    "purpose": "Learn how systems actually behave under load and why assumptions made at low traffic often collapse at scale.",
    "details": "A controlled load-testing harness that applies increasing traffic to a simple backend service and captures latency, error rates, and saturation signals. The lab focuses on correlating load levels with performance degradation and identifying bottlenecks using metrics rather than intuition.",
    "outcome": "Ability to design meaningful load tests, interpret results correctly, and identify true system bottlenecks instead of guessing.",
    "topicsCovered": ["T12"],
    "techStack": {
      "runtime": ["Node.js"],
      "tools": ["k6 or Artillery"],
      "libraries": [],
      "optional": []
    },
    "features": [
      {
        "featureId": "MP5-F1",
        "name": "Baseline service under test",
        "description": "A minimal backend endpoint used as the target for load testing.",
        "topicsCovered": ["T12"],
        "implementationNotes": [
          "Keep logic simple and deterministic",
          "Expose a single primary endpoint for testing"
        ],
        "pitfallsToAvoid": [
          "Testing against complex business logic",
          "Changing code while running comparisons"
        ]
      },
      {
        "featureId": "MP5-F2",
        "name": "Incremental load profiles",
        "description": "Apply gradually increasing traffic to observe system behavior.",
        "topicsCovered": ["T12"],
        "implementationNotes": [
          "Define step-based and ramp-up load patterns",
          "Hold load steady long enough to observe saturation"
        ],
        "pitfallsToAvoid": [
          "Jumping directly to extreme load",
          "Not allowing systems to stabilize"
        ]
      },
      {
        "featureId": "MP5-F3",
        "name": "Latency and error measurement",
        "description": "Measure response times, percentiles, and error rates under load.",
        "topicsCovered": ["T12"],
        "implementationNotes": [
          "Track p50, p95, and p99 latencies",
          "Separate client errors from server errors"
        ],
        "pitfallsToAvoid": ["Using averages only", "Ignoring tail latency"]
      },
      {
        "featureId": "MP5-F4",
        "name": "Saturation detection",
        "description": "Identify the point at which the system becomes saturated.",
        "topicsCovered": ["T12"],
        "implementationNotes": [
          "Observe queue growth, response time spikes, and error rates",
          "Correlate metrics to find the bottleneck"
        ],
        "pitfallsToAvoid": [
          "Assuming CPU is always the bottleneck",
          "Ignoring downstream dependencies"
        ]
      },
      {
        "featureId": "MP5-F5",
        "name": "Before/after optimization comparison",
        "description": "Apply a simple optimization and compare load test results.",
        "topicsCovered": ["T12"],
        "implementationNotes": [
          "Change only one variable at a time",
          "Re-run identical load profiles"
        ],
        "pitfallsToAvoid": [
          "Multiple simultaneous changes",
          "Uncontrolled test conditions"
        ]
      }
    ],
    "blog": {
      "title": "How load reveals lies",
      "gist": "Show how a system that looks fine under light traffic collapses under load, how to read latency percentiles and saturation signals, and how evidence-based performance tuning differs from guesswork."
    },
    "executionGuidelines": [
      "Always define what success looks like before running tests",
      "Use percentiles, not averages",
      "Hold load steady long enough to see saturation",
      "Change one variable at a time when optimizing"
    ],
    "successCriteria": [
      "Can design realistic load profiles",
      "Can identify saturation points using metrics",
      "Can explain why latency degrades before errors appear",
      "Can validate performance improvements with evidence"
    ],
    "status": {
      "started": false,
      "completed": false,
      "lastUpdated": null
    }
  },
  {
    "miniProjectId": "MP6",
    "name": "API Versioning Playground",
    "type": "lab",
    "purpose": "Learn how APIs break in real systems and how to design versioning and contracts that survive change.",
    "details": "A controlled API playground that exposes multiple versions of the same endpoint and intentionally introduces breaking and non-breaking changes. The lab compares versioning strategies (URL, headers), demonstrates backward compatibility pitfalls, and validates changes using OpenAPI specs and contract tests.",
    "outcome": "Ability to design APIs that evolve safely, recognize breaking changes early, and communicate contract guarantees clearly to consumers.",
    "topicsCovered": ["T25"],
    "techStack": {
      "runtime": ["Node.js"],
      "tools": ["OpenAPI (Swagger)", "Postman or Insomnia"],
      "libraries": ["Express"],
      "optional": []
    },
    "features": [
      {
        "featureId": "MP6-F1",
        "name": "Multiple API versions",
        "description": "Expose the same resource via multiple versions (e.g., v1 and v2).",
        "topicsCovered": ["T25"],
        "implementationNotes": [
          "Support URL-based versioning (/api/v1, /api/v2)",
          "Optionally demonstrate header-based versioning",
          "Keep implementations isolated per version"
        ],
        "pitfallsToAvoid": [
          "Sharing handlers across versions",
          "Implicit version behavior"
        ]
      },
      {
        "featureId": "MP6-F2",
        "name": "Breaking vs non-breaking changes",
        "description": "Demonstrate what constitutes a breaking change in practice.",
        "topicsCovered": ["T25"],
        "implementationNotes": [
          "Additive changes (new optional fields)",
          "Breaking changes (removed fields, changed semantics)",
          "Document expected client impact"
        ],
        "pitfallsToAvoid": [
          "Assuming additive changes are always safe",
          "Ignoring client parsing behavior"
        ]
      },
      {
        "featureId": "MP6-F3",
        "name": "OpenAPI specs per version",
        "description": "Maintain explicit OpenAPI specifications for each API version.",
        "topicsCovered": ["T25"],
        "implementationNotes": [
          "One spec file per version",
          "Generate docs from spec",
          "Use schema diffs to visualize changes"
        ],
        "pitfallsToAvoid": [
          "Single spec covering multiple versions",
          "Specs drifting from implementation"
        ]
      },
      {
        "featureId": "MP6-F4",
        "name": "Backward compatibility testing",
        "description": "Verify that older clients continue to function against newer versions.",
        "topicsCovered": ["T25"],
        "implementationNotes": [
          "Simulate older client expectations",
          "Run contract checks against newer versions"
        ],
        "pitfallsToAvoid": [
          "Manual testing only",
          "Assuming compatibility without verification"
        ]
      },
      {
        "featureId": "MP6-F5",
        "name": "Deprecation signaling",
        "description": "Communicate API deprecation clearly to clients.",
        "topicsCovered": ["T25"],
        "implementationNotes": [
          "Add deprecation headers",
          "Document sunset timelines",
          "Return warnings without breaking clients"
        ],
        "pitfallsToAvoid": [
          "Silent deprecations",
          "Immediate removals without notice"
        ]
      }
    ],
    "blog": {
      "title": "How APIs break silently",
      "gist": "Show real examples of API changes that seem harmless but break clients, how versioning strategies mitigate damage, and why contracts—not implementations—are the true API."
    },
    "executionGuidelines": [
      "Treat API contracts as immutable promises",
      "Assume unknown clients with strict parsers",
      "Version for behavior changes, not implementation convenience",
      "Validate changes using specs and tests, not assumptions"
    ],
    "successCriteria": [
      "Can identify breaking vs non-breaking API changes",
      "Can design a safe versioning strategy",
      "Can explain why backward compatibility is fragile",
      "Can maintain parallel API versions without confusion"
    ],
    "status": {
      "started": false,
      "completed": false,
      "lastUpdated": null
    }
  },
  {
    "miniProjectId": "MP7",
    "name": "Rate Limiter",
    "type": "lab",
    "purpose": "Understand how different rate limiting strategies work, where they fail, and how they affect real users and systems.",
    "details": "A focused lab that implements and compares multiple rate limiting algorithms (fixed window, sliding window, token bucket, leaky bucket). The lab demonstrates fairness, burst handling, and failure behavior under high traffic and adversarial conditions.",
    "outcome": "Ability to choose and justify appropriate rate limiting strategies and understand their tradeoffs in distributed systems.",
    "topicsCovered": ["T22"],
    "techStack": {
      "runtime": ["Node.js"],
      "tools": ["HTTP client (curl or simple script)"],
      "libraries": ["Express"],
      "optional": ["Redis (for shared state simulation)"]
    },
    "features": [
      {
        "featureId": "MP7-F1",
        "name": "Fixed window rate limiter",
        "description": "Implement a basic fixed window counter rate limiter.",
        "topicsCovered": ["T22"],
        "implementationNotes": [
          "Use time-based buckets",
          "Reset counters at window boundaries",
          "Apply per-client limits"
        ],
        "pitfallsToAvoid": [
          "Ignoring burstiness at window edges",
          "Assuming fairness"
        ]
      },
      {
        "featureId": "MP7-F2",
        "name": "Sliding window rate limiter",
        "description": "Smooth request limits by tracking requests over a rolling window.",
        "topicsCovered": ["T22"],
        "implementationNotes": [
          "Track timestamps per request",
          "Calculate rate over recent window"
        ],
        "pitfallsToAvoid": ["High memory usage", "Unbounded data structures"]
      },
      {
        "featureId": "MP7-F3",
        "name": "Token bucket algorithm",
        "description": "Allow controlled bursts while enforcing average rate limits.",
        "topicsCovered": ["T22"],
        "implementationNotes": [
          "Refill tokens at fixed rate",
          "Consume tokens per request"
        ],
        "pitfallsToAvoid": [
          "Incorrect refill logic",
          "Token leakage under concurrency"
        ]
      },
      {
        "featureId": "MP7-F4",
        "name": "Leaky bucket algorithm",
        "description": "Smooth traffic by enforcing a constant outflow rate.",
        "topicsCovered": ["T22"],
        "implementationNotes": ["Queue requests", "Drain at fixed rate"],
        "pitfallsToAvoid": [
          "Unbounded queues",
          "Ignoring queue overflow behavior"
        ]
      },
      {
        "featureId": "MP7-F5",
        "name": "Distributed rate limiting simulation",
        "description": "Simulate rate limiting across multiple instances.",
        "topicsCovered": ["T22"],
        "implementationNotes": [
          "Share counters via Redis or in-memory simulation",
          "Demonstrate consistency issues"
        ],
        "pitfallsToAvoid": [
          "Assuming single-instance correctness scales",
          "Ignoring synchronization costs"
        ]
      }
    ],
    "blog": {
      "title": "Rate limiting strategies compared",
      "gist": "Compare rate limiting algorithms using real behavior: fairness, burst handling, memory cost, and failure modes in distributed systems."
    },
    "executionGuidelines": [
      "Implement one algorithm at a time",
      "Test bursty and steady traffic separately",
      "Simulate distributed behavior explicitly",
      "Measure and compare fairness and accuracy"
    ],
    "successCriteria": [
      "Can explain differences between rate limiting algorithms",
      "Can justify algorithm choice based on traffic patterns",
      "Can demonstrate distributed rate limiting challenges",
      "Can explain how rate limiting protects systems"
    ],
    "status": {
      "started": false,
      "completed": false,
      "lastUpdated": null
    }
  }
]
